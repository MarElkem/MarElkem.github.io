[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Mini-Project 1: Maps (Static)",
    "section": "",
    "text": "For this miniproject, we will be looking at data collected by the Consumer Financial Protection Bureau of complaints about consumer financial products and services from 06/01/24 to 09/10/24\n\n#use the states dataset for interactive map plots and rename the column \"name\" which indicates state in order to join with our variable dataset.\nstates &lt;- states %&gt;%\n  rename(region = name) %&gt;%               \n  mutate(region = tolower(region))        \n\nstates  \n\n\n#rename all states in our complaint dataset to match with us_states and states\ncomplaints &lt;- complaints |&gt; \n   mutate(region = case_when(\n    State == \"AL\" ~ \"alabama\", State == \"AK\" ~ \"alaska\", State == \"AZ\" ~ \"arizona\",\n    State == \"AR\" ~ \"arkansas\", State == \"CA\" ~ \"california\", State == \"CO\" ~ \"colorado\",\n    State == \"CT\" ~ \"connecticut\", State == \"DE\" ~ \"delaware\", State == \"FL\" ~ \"florida\",\n    State == \"GA\" ~ \"georgia\", State == \"HI\" ~ \"hawaii\", State == \"ID\" ~ \"idaho\",\n    State == \"IL\" ~ \"illinois\", State == \"IN\" ~ \"indiana\", State == \"IA\" ~ \"iowa\",\n    State == \"KS\" ~ \"kansas\", State == \"KY\" ~ \"kentucky\", State == \"LA\" ~ \"louisiana\",\n    State == \"ME\" ~ \"maine\", State == \"MD\" ~ \"maryland\", State == \"MA\" ~ \"massachusetts\",\n    State == \"MI\" ~ \"michigan\", State == \"MN\" ~ \"minnesota\", State == \"MS\" ~ \"mississippi\",\n    State == \"MO\" ~ \"missouri\", State == \"MT\" ~ \"montana\", State == \"NE\" ~ \"nebraska\",\n    State == \"NV\" ~ \"nevada\", State == \"NH\" ~ \"new hampshire\", State == \"NJ\" ~ \"new jersey\",\n    State == \"NM\" ~ \"new mexico\", State == \"NY\" ~ \"new york\", State == \"NC\" ~ \"north carolina\",\n    State == \"ND\" ~ \"north dakota\", State == \"OH\" ~ \"ohio\", State == \"OK\" ~ \"oklahoma\",\n    State == \"OR\" ~ \"oregon\", State == \"PA\" ~ \"pennsylvania\", State == \"RI\" ~ \"rhode island\",\n    State == \"SC\" ~ \"south carolina\", State == \"SD\" ~ \"south dakota\", State == \"TN\" ~ \"tennessee\",\n    State == \"TX\" ~ \"texas\", State == \"UT\" ~ \"utah\", State == \"VT\" ~ \"vermont\",\n    State == \"VA\" ~ \"virginia\", State == \"WA\" ~ \"washington\", State == \"WV\" ~ \"west virginia\",\n    State == \"WI\" ~ \"wisconsin\", State == \"WY\" ~ \"wyoming\"\n  )) \n\n\n#create our first subset of our dataset: most complaining states!\n\ncomplaints_summary &lt;- complaints %&gt;% \n  group_by(region) %&gt;% \n  summarize(total_complaints = n())\n\ncomplaints_summary &lt;- complaints_summary %&gt;%  # save it and join with us_states for static plot\n  left_join(us_states, by = \"region\")\n\n\n\nPLOT 1\n\n\n\nlbeel &lt;- complaints_summary %&gt;% # save our labels\n  group_by(region) %&gt;%\n  summarize(long = mean(long), lat = mean(lat)) %&gt;% # center the labels on the middle of every state\n  mutate(region = as.factor(region))\n\nstatic1 &lt;- complaints_summary |&gt;\n  ggplot(mapping = aes(x = long, y = lat)) + \n  geom_polygon(aes(fill = total_complaints, group = group), color = \"black\") + \n  labs(\n    fill = \"Number of Complaints\",\n    caption = \"Data Source: https://www.consumerfinance.gov/\n    Consumer Financial Protection Bureau \n    (Complaints from 06/01/2024 - 09/10/2024)\"\n  ) +\n  coord_map() + \n  scale_fill_viridis_c() +\n  geom_text(\n    data = lbeel, \n    mapping = aes(x = long, y = lat, label = region), \n    size = 3, \n    color = \"white\"\n  )\n\nstatic1\n\n\n\n\n\n\n\n\nWe have a US state map showing the number of complaints from 06/01/24 to 09/10/24 against companies that were gathered on The Consumer Complaint Database- a collection of complaints about consumer financial products and services. We can see color-coding for each state with a legend on the right showing us the ranges of the different color assignments from purple(0-20k complaints)-blue(20-40k complaints)-teal(30-60k complaints)-green/yellow(60-80k+ complaints). All the states are purple except 7 states. In order of most complaints:\nYellow : Florida (80k), Texas (~70k) Green: California (60K), Georgia (50k) Blue: Illinois (35k), Pennsylvania (30K), New York (40k)\nThe graph shows that the highest number of complaints is coming from Flordia, and the least coming from Alabama (10k) and the average complaint number per state is 20k\n\n\nPLOT 2\n\n\n\nlbeel2 &lt;- complaints_summary %&gt;%\n  group_by(region) %&gt;%\n  summarize(long = mean(long), lat = mean(lat)) %&gt;%\n  mutate(region = as.factor(region))\n\nstatic2 &lt;- staticcompany |&gt;\n  ggplot(mapping = aes(x = long, y = lat)) + \n  geom_polygon(aes(fill = Company, group = group), color = \"black\") + \n  labs(\n    fill = \"Company Complained About\",\n    caption = \"Data Source: https://www.consumerfinance.gov/\n    Consumer Financial Protection Bureau \n    (Complaints from 06/01/2024 - 09/10/2024)\"\n  ) +\n  coord_map() + \n  scale_fill_viridis_d() +\n  geom_text(\n    data = lbeel2, \n    mapping = aes(x = long, y = lat, label = region), \n    size = 3, \n    color = \"white\"\n  )\n\nstatic2\n\n\n\n\n\n\n\n\nOur plot informs us of the 3 most complained about companies- which are Transunion (a credit reporting agency headquarted in Chicago, Illinois), (Experian is a multinational data analytics and consumer credit reporting company headquartered in Dublin, Ireland), and Equifax (a consumer credit reporting agency headquartered in Atlanta, Georgia), Experian Information Solutions. The States who complain the most about Equifax are WA, OR, ID, WY, NE, KA, MI, TE, KE, WV, OH, NC, VI, MS, AL, LO. For Experian, it is MO and ND only- and all the rest are TransUnion."
  },
  {
    "objectID": "about2.html",
    "href": "about2.html",
    "title": "Mini-Project 1: Maps (Interactive)",
    "section": "",
    "text": "This is a continuation of MP1, where we have the same insights and data but in an interactive form!\n\n#rename all states in our complaint dataset to match with us_states and states\ncomplaints &lt;- complaints |&gt; \n   mutate(region = case_when(\n    State == \"AL\" ~ \"alabama\", State == \"AK\" ~ \"alaska\", State == \"AZ\" ~ \"arizona\",\n    State == \"AR\" ~ \"arkansas\", State == \"CA\" ~ \"california\", State == \"CO\" ~ \"colorado\",\n    State == \"CT\" ~ \"connecticut\", State == \"DE\" ~ \"delaware\", State == \"FL\" ~ \"florida\",\n    State == \"GA\" ~ \"georgia\", State == \"HI\" ~ \"hawaii\", State == \"ID\" ~ \"idaho\",\n    State == \"IL\" ~ \"illinois\", State == \"IN\" ~ \"indiana\", State == \"IA\" ~ \"iowa\",\n    State == \"KS\" ~ \"kansas\", State == \"KY\" ~ \"kentucky\", State == \"LA\" ~ \"louisiana\",\n    State == \"ME\" ~ \"maine\", State == \"MD\" ~ \"maryland\", State == \"MA\" ~ \"massachusetts\",\n    State == \"MI\" ~ \"michigan\", State == \"MN\" ~ \"minnesota\", State == \"MS\" ~ \"mississippi\",\n    State == \"MO\" ~ \"missouri\", State == \"MT\" ~ \"montana\", State == \"NE\" ~ \"nebraska\",\n    State == \"NV\" ~ \"nevada\", State == \"NH\" ~ \"new hampshire\", State == \"NJ\" ~ \"new jersey\",\n    State == \"NM\" ~ \"new mexico\", State == \"NY\" ~ \"new york\", State == \"NC\" ~ \"north carolina\",\n    State == \"ND\" ~ \"north dakota\", State == \"OH\" ~ \"ohio\", State == \"OK\" ~ \"oklahoma\",\n    State == \"OR\" ~ \"oregon\", State == \"PA\" ~ \"pennsylvania\", State == \"RI\" ~ \"rhode island\",\n    State == \"SC\" ~ \"south carolina\", State == \"SD\" ~ \"south dakota\", State == \"TN\" ~ \"tennessee\",\n    State == \"TX\" ~ \"texas\", State == \"UT\" ~ \"utah\", State == \"VT\" ~ \"vermont\",\n    State == \"VA\" ~ \"virginia\", State == \"WA\" ~ \"washington\", State == \"WV\" ~ \"west virginia\",\n    State == \"WI\" ~ \"wisconsin\", State == \"WY\" ~ \"wyoming\"\n  )) \n\n\n#use the states dataset for interactive map plots and rename the column \"name\" which indicates state in order to join with our variable dataset.\nstates &lt;- states %&gt;%\n  rename(region = name) %&gt;%               \n  mutate(region = tolower(region))        \n\n\n#create our first subset of our dataset: most complaining states!\n\ncomplaints_summary &lt;- complaints %&gt;% \n  group_by(region) %&gt;% \n  summarize(total_complaints = n())\n\n\n# save it and join with us_states for interactive plot\ncomplaints_geo &lt;- complaints_summary %&gt;%\n  left_join(states, by = \"region\") %&gt;% \n  st_as_sf()\n\n\nleaflet(complaints_geo) %&gt;%\n  addTiles() %&gt;%\n  addPolygons(\n    fillColor = ~colorNumeric(\"viridis\", complaints_geo$total_complaints)(total_complaints), #we want to show the variable \"total complaints\"\n    weight = 1,\n    color = \"black\",\n    fillOpacity = 0.7,\n    label = ~paste(region, \"Complaints:\", total_complaints) #pop-up label format\n  ) %&gt;%\n  addLegend(\n    pal = colorNumeric(\"viridis\", complaints_geo$total_complaints), \n    values = complaints_geo$total_complaints,\n    title = \"Number of Complaints\",\n    position = \"bottomright\"\n  ) %&gt;% \n  addControl(\n    html = \"Data Source: &lt;a href='https://www.consumerfinance.gov/'&gt;Consumer Financial Protection Bureau&lt;/a&gt; (Complaints from 06/01/2024 - 09/10/2024)\", \n    position = \"bottomleft\"\n  )\n\n\n\n\n\nHover over the states to see how many complaints they have filed!\n\n#second interactive plot, most complained about companies!\ncomplaints_company2 &lt;- complaints %&gt;%\n  group_by(region, Company) %&gt;%\n  summarize(complaint_count = n(), .groups = \"drop\") %&gt;%  \n  slice_max(order_by = complaint_count, n = 1, by=region, with_ties = FALSE)  #get the most complained about company for every state\n\n\n#join it to our interactive mapping dataset, state\ninteractivecompany &lt;- complaints_company2 %&gt;% \n  left_join(states, by=\"region\") %&gt;% \n    st_as_sf()\n\n\nbinColor &lt;- colorFactor(palette = \"Set3\", domain = interactivecompany$Company)\n\ninteractivecompany &lt;- interactivecompany %&gt;%\n  mutate(labels = lapply(paste0(\"&lt;strong&gt;State: \", region, \"&lt;/strong&gt;&lt;br/&gt;\", #create our labels\n                                \"Company: \", Company, \"&lt;br/&gt;\", \n                                \"Complaints: \", complaint_count), \n                         HTML))\n\nleaflet(interactivecompany) %&gt;%\n  setView(lng = -96, lat = 37.8, zoom = 4) %&gt;%  #focus the map on the US\n  addTiles() %&gt;%\n  addPolygons(\n    weight = 2,\n    opacity = 1,\n    fillColor = ~binColor(Company),  \n    color = \"black\",\n    dashArray = \"3\",\n    fillOpacity = 0.7,\n    highlightOptions = highlightOptions(\n      weight = 5,\n      color = \"#666\",\n      dashArray = \"\",\n      fillOpacity = 0.7,\n      bringToFront = TRUE\n    ),\n    label = ~labels,\n    labelOptions = labelOptions(\n      style = list(\"font-weight\" = \"normal\", padding = \"3px 8px\"),\n      textsize = \"15px\",\n      direction = \"auto\"\n    )\n  ) %&gt;% \n  addControl(\n    html = \"Data Source: &lt;a href='https://www.consumerfinance.gov/'&gt;Consumer Financial Protection \n    Bureau&lt;/a&gt; (Complaints from 06/01/2024 - 09/10/2024)\", \n    position = \"bottomleft\"\n  )\n\n\n\n\n\nHover over the states to see what company they have complained about the most!"
  },
  {
    "objectID": "about3.html",
    "href": "about3.html",
    "title": "Mini-Project 2: Data Acquisition",
    "section": "",
    "text": "Data Source: https://fbref.com/en/comps/1/World-Cup-Stat\nMotivation: as two avid football (soccer) fans. We are hoping to analyze and explore the World Cup 2022 data. we will also potentially explore the last 10 World Cups. We will do so by exploring player statistics, match standings data, and other external variables such as sentiment analysis and audience engagement.\nTwo similar questions we hope to explore are:\n\n\nIf there is a strong correlation between goals scored and points earned in the group stage\n\n\n\n\nDo points, goals scored, goals against, or goal difference have the highest impact on progression towards knockout/semi-finals stages.\n\n\nEthical Justification: We believe this data does not have ethical implications as long as it is scraped politely. One approach we will keep in mind is to clean and analyze various squads to get a representation of the real world rather than just focusing on popular squads\n\n#First we have to make sure scraping is allowed\nrobotstxt::paths_allowed(\"https://fbref.com/en/comps/1/World-Cup-Stats\")\n\n\n fbref.com                      \n\n#Next we scrape the data\nsession &lt;- bow(\"https://fbref.com/en/comps/1/World-Cup-Stats\")\n\n\n# Next we will store our session \nwc22 &lt;- scrape(session)\n\n# Next we will save our tables\ntables &lt;- html_nodes(wc22, css = \"table\")\n\n# Get all the groups we need\nhtml_table(tables, header = TRUE, fill = TRUE)   \nwc_group1 &lt;- html_table(tables, header = TRUE, fill = TRUE)[[1]]  \nwc_group2 &lt;- html_table(tables, header = TRUE, fill = TRUE)[[2]]  \nwc_group3 &lt;- html_table(tables, header = TRUE, fill = TRUE)[[3]]  \nwc_group4 &lt;- html_table(tables, header = TRUE, fill = TRUE)[[4]]  \nwc_group5 &lt;- html_table(tables, header = TRUE, fill = TRUE)[[5]]  \nwc_group6 &lt;- html_table(tables, header = TRUE, fill = TRUE)[[6]]  \nwc_group7 &lt;- html_table(tables, header = TRUE, fill = TRUE)[[7]]  \nwc_group8 &lt;- html_table(tables, header = TRUE, fill = TRUE)[[8]]  \n\n\n\nAbove we are scraping data of all the group stages of the 2022 World Cup\n\n\n\nall_groups &lt;- bind_rows(lapply(list(wc_group1, wc_group2, wc_group3, wc_group4,\n                                      wc_group5, wc_group6, wc_group7, wc_group8), function(group) {\n  # Rename columns\n  colnames(group) &lt;- c(\"Rank\", \"Squad\", \"Matches Played\", \"Wins\", \"Draws\", \"Losses\",\n                       \"Goals For\", \"Goals Against\", \"Goal Difference\", \"Points\")\n  \n  # Remove the first two-letter abbreviation from Squad names\n  group$Squad &lt;- str_replace(group$Squad, \"^[^ ]+ \", \"\")\n  \n  # Remove the xGD/90 column if it exists\n  if (\"xGD/90\" %in% colnames(group)) {\n    group &lt;- select(group, -`xGD/90`)\n  }\n  \n  return(group)\n}))\n\nNew names:\nNew names:\nNew names:\nNew names:\nNew names:\nNew names:\nNew names:\nNew names:\n• `` -&gt; `...11`\n• `` -&gt; `...12`\n• `` -&gt; `...13`\n• `` -&gt; `...14`\n• `` -&gt; `...15`\n\n# Display the combined tibble\nall_groups\n\n# A tibble: 32 × 15\n    Rank Squad   `Matches Played`  Wins Draws Losses `Goals For` `Goals Against`\n   &lt;int&gt; &lt;chr&gt;              &lt;int&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt;       &lt;int&gt;           &lt;int&gt;\n 1     1 Nether…                3     2     1      0           5               1\n 2     2 Senegal                3     2     0      1           5               4\n 3     3 Ecuador                3     1     1      1           4               3\n 4     4 Qatar                  3     0     0      3           1               7\n 5     1 England                3     2     1      0           9               2\n 6     2 United…                3     1     2      0           2               1\n 7     3 IR Iran                3     1     0      2           4               7\n 8     4 Wales                  3     0     1      2           1               6\n 9     1 Argent…                3     2     0      1           5               2\n10     2 Poland                 3     1     1      1           2               2\n# ℹ 22 more rows\n# ℹ 7 more variables: `Goal Difference` &lt;int&gt;, Points &lt;int&gt;, ...11 &lt;dbl&gt;,\n#   ...12 &lt;dbl&gt;, ...13 &lt;dbl&gt;, ...14 &lt;dbl&gt;, ...15 &lt;chr&gt;\n\n\n\n\nAbove we have used a for loop to clean the data which included changing the names of colums and filtering. We now have all the 8 group stages statistics for the 2022 WC which include: “Rank”, “Squad”, “Matches Played”, “Wins”, “Draws”, “Losses”, “Goals For”, “Goals Against”, “Goal Difference”, “Points”. We also combined all the groups into a single tibble.\n\n\nIn conclusion, we hope to create interesting data visualizations as well as find our which metrics affect the probability of entering the knockout/semi-final/advanced stages. We will also later on add more data such as player statistics to see any further patterns in the data"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MP3 DS2",
    "section": "",
    "text": "Maroova Elkemary\n\n\n\n\nHi! My name is Maroova Elkemary. I am a data enthusiast and love to combine data science with different realms such as video games, finance, sports, and music!\n\n\n\n\nI have had two data analysis work experiences so far where I worked with SQL, Python, and mostly focused on machine learning. I will show you a few of my data science projects so far in my school work!\n\n\n   GitHub Profile"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "MP3 DS2",
    "section": "",
    "text": "Maroova Elkemary\n\n\n\n\nHi! My name is Maroova Elkemary. I am a data enthusiast and love to combine data science with different realms such as video games, finance, sports, and music!\n\n\n\n\nI have had two data analysis work experiences so far where I worked with SQL, Python, and mostly focused on machine learning. I will show you a few of my data science projects so far in my school work!\n\n\n   GitHub Profile"
  },
  {
    "objectID": "about4.html",
    "href": "about4.html",
    "title": "Mini-Project 4: Text Analysis",
    "section": "",
    "text": "This mini-project explores sentiment, uniqueness, and trends accross Tupac’s songs. The data obtained from https://www.kaggle.com/datasets/leopoldofacci/tupac-dataset-lyrics-analyse.\nThe data originally contained: “Lyrics”, “Song”, “Artist”. I manually added the album names and will add years for time trajectory analysis in the final project.\n\n#Import relevant libraries\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(widyr)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ readr     2.1.5\n✔ ggplot2   3.5.1     ✔ stringr   1.5.1\n✔ lubridate 1.9.3     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(stringr)\nlibrary(tidytext)\nlibrary(DataEditR)\nlibrary(wordcloud2)\nlibrary(wordcloud)\n\nLoading required package: RColorBrewer\n\nlibrary(RColorBrewer)\nlibrary(tidyr)\nlibrary(tidytext)\n\n\n#Read the data, and create a dictionary with regular expressions of cuss-words to filter out later.\ntupac_untidy &lt;- read.csv(\"~/Sds 264 F24/lyrics-TUPAC.csv\")\ncuss &lt;- read.csv(\"~/Sds 264 F24/cuss2.csv\")\ncuss_words_pattern &lt;- paste0(\"\\\\b(\", paste(cuss$word, collapse = \"|\"), \")\\\\b\") \n\n\n#Create an unnested version of the dataset using str functions and regular expressions\ntupac_sep &lt;- tupac_untidy |&gt; \n  unnest_tokens(word, Lyrics) |&gt; \n  anti_join(cuss, by = \"word\")\n\ntupac_tidy &lt;- tupac_untidy |&gt; \n  mutate(Lyrics = str_replace_all(Lyrics, \"\\\\n\", \" \")) \n\n\n#Through regular expressions, find out what are the largest numbers said in a song in the dataset\ntupac_sep |&gt; \n  mutate(number = as.numeric(str_extract(word, \"\\\\b\\\\d+\\\\b\"))) |&gt;  \n  filter(!is.na(number) & Song != \"When I Get Free II\") |&gt;  \n  slice_max(number, n = 10, with_ties = FALSE) |&gt; \n  select(Song, number)\n\n                     Song number\n1                  Outlaw   1995\n2          Soulja's Story   1993\n3      Picture Me Rollin'    500\n4     Picture Me Rollin'*    500\n5  The Streetz R Deathrow    380\n6               Peep Game    357\n7      Picture Me Rollin'    200\n8     Picture Me Rollin'*    200\n9  The Streetz R Deathrow    187\n10         Fuck The World    165\n\n\n\n\nHere we can see that there are a lot of numbers, with some research we can find out that in the song “Outlawz” 2pac says “Things are not the same since 1995”. 1995 is the year when 2pac was released pending an appeal of a conviction in 1995\n\n\n\n\nIn Soulja’s Story, 1993 refers to a line “Straight soldier, 1993 onward”, I am unsure what it means as the song was released in 1991.\n\n\n\n#Find words that are repeating twice in a row using regular expressions\ntupac_tidy |&gt; \n  mutate(repeated_words = str_extract_all(Lyrics, \"\\\\b(\\\\w+)\\\\b[\\\\s]+\\\\1\\\\b\")) |&gt; \n  unnest(repeated_words) |&gt; \n  filter(!is.na(repeated_words)) |&gt; \n  distinct(Song, repeated_words) |&gt; \n  select(Song, repeated_words)\n\n# A tibble: 45 × 2\n   Song                     repeated_words\n   &lt;chr&gt;                    &lt;chr&gt;         \n 1 Hail Mary                la la         \n 2 Keep Ya Head Up          you you       \n 3 Thugz Mansion (Acoustic) where where   \n 4 Hit 'Em Up               ha ha         \n 5 Hit 'Em Up               Ha Ha         \n 6 Hit 'Em Up               four four     \n 7 I Get Around             bye bye       \n 8 Keep Your Head Up        you you       \n 9 Me And My Girlfriend     uh uh         \n10 Until The End Of Time    like like     \n# ℹ 35 more rows\n\n\n\n\nSome of the twice repeating words are silly and some are serious, for example “La la” and “murder murder”. This shows the duality of 2pac.\n\n\n\n#Through str functions, find out longest words:\ntupac_sep |&gt; \n  distinct(word, .keep_all = TRUE) |&gt;  # Keep only distinct words\n  mutate(word_length = str_length(word)) |&gt;  # Calculate the length of each word\n  filter(Song!=\"All Eyez On Me\") |&gt; \n  slice_max(word_length, n = 10, with_ties = FALSE)  # Select the top 10 longest words without ties\n\n   Artist                      Song                 Album               word\n1    2pac              Young Niggaz  Me Against The World youknowhati'msayin\n2    2pac                 Hail Mary Hip-Hop Classics 1996  institutionalized\n3    2pac     Only God Can Judge Me        All Eyez On Me  yaknowhati'msayin\n4    2pac               Papa'z Song         Strictly 4 My   responsibilities\n5    2pac 2 Of Amerikaz Most Wanted        All Eyez On Me    multimillionare\n6    2pac           5 Deadly Venomz         Strictly 4 My    knowhati'msayin\n7    2pac         Ain't Hard 2 Find        All Eyez On Me    youknowhatimean\n8    2pac                   Changes         Greatest Hits     penitentiary's\n9    2pac                 Hail Mary Hip-Hop Classics 1996     penitentiaries\n10   2pac        I Ain't Mad At Cha        All Eyez On Me     congratulation\n   word_length\n1           18\n2           17\n3           17\n4           16\n5           15\n6           15\n7           15\n8           14\n9           14\n10          14\n\n\n\n\nWhat could be the longest words in old-school rap? We see a famous catchphrase of 2pac which is yaknowhati’msayin appearing in different songs, other words include: responsibilities, institutionalized, multimillionare, penitentiary, and congratulation. If anything- we can sense repeating themes of worry, money, and lifestyle.\n\n\n\n#Analyze the sentiment of positivity vs negativity in songs across albums\nbing_sentiment &lt;- get_sentiments(\"bing\")\n\nsentiment_scores &lt;- tupac_sep |&gt; \n  filter(!is.na(word)) |&gt; \n  inner_join(bing_sentiment, by = \"word\") |&gt;  #join sentiments and lyrics\n  group_by(Album, sentiment) |&gt; \n  summarize(sentiment_count = n(), .groups = 'drop') |&gt; \n  ungroup()\n\nWarning in inner_join(filter(tupac_sep, !is.na(word)), bing_sentiment, by = \"word\"): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 37094 of `x` matches multiple rows in `y`.\nℹ Row 3682 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\nggplot(sentiment_scores, aes(x = Album, y = sentiment_count, fill = sentiment)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(\n    title = \"Positive and Negative Sentiment of Tupac Albums\",\n    x = \"Album\",\n    y = \"Sentiment Count\",\n    fill = \"Sentiment\"\n  ) +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\n  scale_fill_manual(values = c(\"positive\" = \"blue\", \"negative\" = \"black\")) +\n  scale_y_continuous(expand = expansion(mult = c(0, 0.1)))\n\n\n\n\n\n\n\nprint(sentiment_scores)\n\n# A tibble: 28 × 3\n   Album                 sentiment sentiment_count\n   &lt;chr&gt;                 &lt;chr&gt;               &lt;int&gt;\n 1 2Pacalypse Now        negative              362\n 2 2Pacalypse Now        positive              137\n 3 All Eyez On Me        negative              718\n 4 All Eyez On Me        positive              345\n 5 Better Dayz           negative              184\n 6 Better Dayz           positive              219\n 7 Greatest Hits         negative              164\n 8 Greatest Hits         positive               62\n 9 Hip-Hop Classics 1996 negative               53\n10 Hip-Hop Classics 1996 positive               35\n# ℹ 18 more rows\n\n\n\n\nAbnove, we can see the album with the highest positivity and negativity is All Eyez On Me. This album contains versatile songs and lyrics and as such it is not a surprise to see that it has the highest count of both sentiments.\n\n\n\n#Find the fear sentiment in 2pac songs and create a wordcloud\nnrc_sentiment &lt;- get_sentiments(\"nrc\")\n\ntupac_nrc &lt;- tupac_sep|&gt; \n  filter(!is.na(word))|&gt; \n  inner_join(nrc_sentiment, by = \"word\")|&gt; \n  filter(sentiment == \"fear\")|&gt;   # Focus only on fear sentiment\n  count(Album, word, sort = TRUE)|&gt;  \n  distinct(word, .keep_all = TRUE)|&gt;   # Remove duplicates per album\n  ungroup()\n\nWarning in inner_join(filter(tupac_sep, !is.na(word)), nrc_sentiment, by = \"word\"): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 24 of `x` matches multiple rows in `y`.\nℹ Row 7349 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\ntop_fear_words &lt;- tupac_nrc|&gt; \n  group_by(Album)|&gt; \n  slice_max(n, n = 7)|&gt;   \n  ungroup()\n\nwordcloud(\n  words = top_fear_words$word, \n  freq = top_fear_words$n, \n  max.words = 200,  \n  random.order = FALSE,  \n  rot.per = 0.2,  \n  scale = c(4, 0.5), \n  colors = brewer.pal(8, \"Set1\")  \n)\n\n\n\n\n\n\n\n\n\n\nThe wordcloud above highlights words with a fearing sentiment to them. Some words include “hate, cemetery, wicked, and god as the highest fear sentiment.\n\n\n\n#A tf-idf plot with bigrams per album\ntupac_ngram &lt;- tupac_tidy |&gt;\n  unnest_tokens(word, Lyrics) |&gt; \n  anti_join(stop_words, by = \"word\") |&gt;  # Remove stop words\n  anti_join(cuss, by = \"word\") |&gt;  # Remove cuss words from the 'cuss' dataset\n  group_by(Song, Album) |&gt;  # Group by both Song and Album\n  summarise(Lyrics = str_c(word, collapse = \" \"), .groups = \"drop\") |&gt;  # Collapse words into single Lyrics column\n  unnest_tokens(bigram, Lyrics, token = \"ngrams\", n = 2) |&gt;  # Create bigrams\n  filter(!is.na(bigram))  # Filter out NA bigrams\n\n# Calculating TF-IDF and visualizing\nbigram_tf_idf &lt;- tupac_ngram |&gt; \n  count(Album, bigram, sort = TRUE) |&gt; \n  bind_tf_idf(bigram, Album, n)\n\nbigram_tf_idf |&gt;\n  group_by(Album) |&gt;  # Group by Album for analysis\n  arrange(desc(tf_idf)) |&gt;  # Arrange in descending order of tf_idf\n  slice_max(tf_idf, n = 10, with_ties = FALSE) |&gt;  # Select top 10 bigrams per Album\n  ungroup() |&gt;  \n  ggplot(aes(x = fct_reorder(bigram, tf_idf), y = tf_idf, fill = Album)) +  # Reorder bigrams based on tf_idf\n    geom_col(show.legend = FALSE) +  # Create bar plot\n    coord_flip() +  # Flip coordinates for horizontal bars\n    facet_wrap(~Album, scales = \"free\")\n\n\n\n\n\n\n\n\n\n\nAccording to the td-idf statistic and analysis we can see a lot of 2pac’s uniqueness in lyrics comes from themes of identity and even location (as California appears multpile times). For example, in 2Pacalypse now, 2Pac highlights his racial identity in sensitive detail. While “Greatest Hits” includes more lifestyle songs.\n\n\n#In conclusion, Tupac’s music has a lot of versatality- with a wide variety of emotions and topics."
  }
]